{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# LeJEPA Edge Captioner - Training Notebook\n",
                "\n",
                "Train the VL-JEPA style embedding prediction model on COCO Captions.\n",
                "\n",
                "**Requirements:**\n",
                "- Kaggle GPU (T4/P100)\n",
                "- COCO dataset\n",
                "- Gemma-3 access"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install dependencies\n",
                "!pip install -q timm datasets transformers accelerate\n",
                "\n",
                "# Clone the repo (if not already present)\n",
                "!git clone https://github.com/omar-A-hassan/lejepa.git || true\n",
                "%cd lejepa"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import sys\n",
                "sys.path.insert(0, '.')\n",
                "\n",
                "from lejepa_caption.models import LeJEPACaptioner, get_captioner\n",
                "\n",
                "# Check GPU\n",
                "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
                "print(f'Device: {device}')\n",
                "print(f'GPU: {torch.cuda.get_device_name(0) if device == \"cuda\" else \"N/A\"}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Load Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create model (use 'tiny' for faster training)\n",
                "model = get_captioner('small')\n",
                "\n",
                "params = model.num_parameters\n",
                "print(f'Encoder: {params[\"encoder\"] / 1e6:.1f}M')\n",
                "print(f'Connector: {params[\"connector\"] / 1e6:.1f}M')\n",
                "print(f'Predictor: {params[\"predictor\"] / 1e6:.1f}M')\n",
                "print(f'Total: {params[\"total\"] / 1e6:.1f}M')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Load Gemma-3 (for target embeddings)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
                "\n",
                "# Load Gemma-3 (requires accepting license on HuggingFace)\n",
                "LLM_NAME = 'google/gemma-3-270m-it'\n",
                "\n",
                "tokenizer = AutoTokenizer.from_pretrained(LLM_NAME)\n",
                "llm = AutoModelForCausalLM.from_pretrained(\n",
                "    LLM_NAME,\n",
                "    torch_dtype=torch.bfloat16,\n",
                "    device_map='auto',\n",
                ")\n",
                "llm.eval()\n",
                "\n",
                "# Freeze LLM\n",
                "for param in llm.parameters():\n",
                "    param.requires_grad = False\n",
                "\n",
                "print(f'LLM embedding dim: {llm.get_input_embeddings().weight.shape}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Load COCO Dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from datasets import load_dataset\n",
                "from torch.utils.data import DataLoader\n",
                "from torchvision import transforms\n",
                "\n",
                "# Load COCO\n",
                "coco = load_dataset('HuggingFaceM4/COCO', split='train[:10000]')  # Subset for demo\n",
                "\n",
                "# Transforms\n",
                "transform = transforms.Compose([\n",
                "    transforms.Resize(256),\n",
                "    transforms.CenterCrop(224),\n",
                "    transforms.ToTensor(),\n",
                "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
                "])\n",
                "\n",
                "def collate_fn(batch):\n",
                "    images = [transform(item['image'].convert('RGB')) for item in batch]\n",
                "    captions = [item['sentences'][0]['raw'] for item in batch]\n",
                "    return torch.stack(images), captions\n",
                "\n",
                "train_loader = DataLoader(coco, batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
                "print(f'Batches: {len(train_loader)}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Training Loop"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch.nn.functional as F\n",
                "from torch.optim import AdamW\n",
                "from tqdm import tqdm\n",
                "\n",
                "model = model.to(device)\n",
                "optimizer = AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)\n",
                "\n",
                "MAX_LEN = 50\n",
                "EPOCHS = 3\n",
                "\n",
                "for epoch in range(EPOCHS):\n",
                "    model.train()\n",
                "    total_loss = 0\n",
                "    \n",
                "    for images, captions in tqdm(train_loader, desc=f'Epoch {epoch+1}'):\n",
                "        images = images.to(device)\n",
                "        \n",
                "        # Forward\n",
                "        pred_embeds = model(images, num_tokens=MAX_LEN)\n",
                "        \n",
                "        # Get target embeddings\n",
                "        with torch.no_grad():\n",
                "            tokens = tokenizer(\n",
                "                captions, \n",
                "                padding='max_length', \n",
                "                truncation=True, \n",
                "                max_length=MAX_LEN,\n",
                "                return_tensors='pt'\n",
                "            ).to(device)\n",
                "            target_embeds = llm.get_input_embeddings()(tokens.input_ids).float()\n",
                "        \n",
                "        # MSE loss\n",
                "        loss = F.mse_loss(pred_embeds, target_embeds)\n",
                "        \n",
                "        # Backward\n",
                "        optimizer.zero_grad()\n",
                "        loss.backward()\n",
                "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
                "        optimizer.step()\n",
                "        \n",
                "        total_loss += loss.item()\n",
                "    \n",
                "    avg_loss = total_loss / len(train_loader)\n",
                "    print(f'Epoch {epoch+1}: Loss = {avg_loss:.4f}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Save Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save checkpoint\n",
                "torch.save({\n",
                "    'model_state_dict': model.state_dict(),\n",
                "    'config': model.config,\n",
                "}, 'lejepa_captioner.pt')\n",
                "\n",
                "print('Model saved!')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Test Inference"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test with a sample image\n",
                "model.eval()\n",
                "with torch.no_grad():\n",
                "    sample_img = images[:1]\n",
                "    pred = model(sample_img)\n",
                "    print(f'Predicted embedding shape: {pred.shape}')\n",
                "    \n",
                "    # Optional: Decode using LLM\n",
                "    # output = llm.generate(inputs_embeds=pred, max_new_tokens=20)\n",
                "    # print(tokenizer.decode(output[0]))"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
